{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myrandaGoesToSpace/langchain/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a8ac011",
      "metadata": {
        "id": "2a8ac011"
      },
      "source": [
        "# LangChain Examples\n",
        "\n",
        "These examples are from LangChain's [QuickStart Guide](https://python.langchain.com/en/latest/getting_started/getting_started.html)\n",
        "\n",
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running this in Google Colab, you will need to install the libraries using the cell below:"
      ],
      "metadata": {
        "id": "gxLzCsHDFi8j"
      },
      "id": "gxLzCsHDFi8j"
    },
    {
      "cell_type": "code",
      "source": [
        "## ONLY RUN IF USING GOOGLE COLAB ##\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install google-search-results\n",
        "## ##"
      ],
      "metadata": {
        "id": "92eUGxpfFq3F"
      },
      "id": "92eUGxpfFq3F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f9676c",
      "metadata": {
        "id": "76f9676c"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import ConversationChain\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI API key\n",
        "Get an OpenAI [API token](https://platform.openai.com/docs/api-reference) and save it somewhere secure. Change the path to the key in the cell below accordingly, or enter the key manually (not recommended if sharing this notebook with others)."
      ],
      "metadata": {
        "id": "WgGplInnGFFT"
      },
      "id": "WgGplInnGFFT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8681991",
      "metadata": {
        "id": "b8681991"
      },
      "outputs": [],
      "source": [
        "# ONLY USE IF KEY IS SAVED IN FILE\n",
        "\n",
        "# Change this path to your key location\n",
        "path_to_key = \"./openai-api-key.txt\"\n",
        "\n",
        "with open(path_to_key) as fo:\n",
        "    key = fo.readline()\n",
        "    \n",
        "os.environ[\"OPENAI_API_KEY\"] = key.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TO ENTER KEY MANUALLY, DELETE TRIPLE QUOTES AND ENTER KEY \n",
        "''' \n",
        "key = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = key.strip()\n",
        "''' \n",
        "##"
      ],
      "metadata": {
        "id": "M-myC8PUHdLS"
      },
      "id": "M-myC8PUHdLS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7221ad84",
      "metadata": {
        "id": "7221ad84"
      },
      "source": [
        "## Get predictions from language model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose your model\n",
        "LangChain has a list of available [models](https://python.langchain.com/en/latest/modules/models.html). Choose the one that is best for your application."
      ],
      "metadata": {
        "id": "SLaRBtgKI3vY"
      },
      "id": "SLaRBtgKI3vY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c424521",
      "metadata": {
        "id": "6c424521"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71187944",
      "metadata": {
        "id": "71187944",
        "outputId": "64d83bf2-c001-45f2-a46f-be301a7a794b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "BrightSox\n"
          ]
        }
      ],
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006bca95",
      "metadata": {
        "id": "006bca95"
      },
      "source": [
        "## Manage Prompts for LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain allows you to format prompts to better control user input using the PromptTemplate class."
      ],
      "metadata": {
        "id": "9BLm9Pv4JQuC"
      },
      "id": "9BLm9Pv4JQuC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574eacdc",
      "metadata": {
        "id": "574eacdc"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"product\"],\n",
        "    template = \"What is a good name for a company that makes {product}?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a279ce87",
      "metadata": {
        "id": "a279ce87",
        "outputId": "ef71bf9c-b15e-4215-d800-e443f9d93c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is a good name for a company that makes modern clothing?\n"
          ]
        }
      ],
      "source": [
        "new_prompt = prompt.format(product=\"modern clothing\")\n",
        "print(new_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9d0c6a",
      "metadata": {
        "id": "3a9d0c6a",
        "outputId": "d005f883-bcde-4506-ec6c-19e3305c3f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ThreadStyles.\n"
          ]
        }
      ],
      "source": [
        "print(llm(new_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4548164",
      "metadata": {
        "id": "b4548164"
      },
      "source": [
        "## Using chains to create a workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains allow you to combine models and prompts for efficient workflows."
      ],
      "metadata": {
        "id": "zszKItTjJv6P"
      },
      "id": "zszKItTjJv6P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af1e212",
      "metadata": {
        "id": "5af1e212"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede52d9e",
      "metadata": {
        "id": "ede52d9e",
        "outputId": "467ee0ce-fc7e-41a1-f3b7-90baac46c934"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nSplashy Sox.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"colorful socks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb86f047",
      "metadata": {
        "id": "bb86f047"
      },
      "source": [
        "## Agents to Dynamically Call Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents allow the model to make decisions on the tools needed to respond to user input. The below example uses an agent in combination with the Google Search API (SERP) and the math tool llm-math. The model can then use these tools to output the proper response. "
      ],
      "metadata": {
        "id": "uFhdHVGWKPc3"
      },
      "id": "uFhdHVGWKPc3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Key"
      ],
      "metadata": {
        "id": "ntEv8nbPKtpm"
      },
      "id": "ntEv8nbPKtpm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this section you will need a SERP [API key](https://serpapi.com/). Add this key in the same way as you did the OpenAI API key."
      ],
      "metadata": {
        "id": "0-k8y7L_INMQ"
      },
      "id": "0-k8y7L_INMQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d81fb7a",
      "metadata": {
        "id": "5d81fb7a"
      },
      "outputs": [],
      "source": [
        "# Get SerpAPI package and API keys\n",
        "\n",
        "# Change to relevant path\n",
        "path_to_key = \"./serp-api-key.txt\"\n",
        "\n",
        "with open(path_to_key) as fo:\n",
        "    key = fo.readline()\n",
        "\n",
        "os.environ[\"SERPAPI_API_KEY\"] = key.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TO ENTER KEY MANUALLY, DELETE TRIPLE QUOTES AND ENTER KEY \n",
        "''' \n",
        "key = \"\"\n",
        "os.environ[\"SERP_API_KEY\"] = key.strip()\n",
        "''' \n",
        "##"
      ],
      "metadata": {
        "id": "ekW1EMZHIjaW"
      },
      "id": "ekW1EMZHIjaW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools and Agents"
      ],
      "metadata": {
        "id": "SagZL7VhKpFA"
      },
      "id": "SagZL7VhKpFA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose your tools (see available tools [here](https://python.langchain.com/en/latest/modules/agents/tools.html)). In this example we will use SERP and llm-math. The load_tools function allows you to attach the tools to the LLM model."
      ],
      "metadata": {
        "id": "jp9ismcsK1YL"
      },
      "id": "jp9ismcsK1YL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbd6863",
      "metadata": {
        "id": "0dbd6863"
      },
      "outputs": [],
      "source": [
        "# Language model already loaded in previous cells\n",
        "# Now we add tools to the llm\n",
        "\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c897ee",
      "metadata": {
        "id": "05c897ee"
      },
      "outputs": [],
      "source": [
        "# initialize agent with tools, llm, and agent type\n",
        "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f17084",
      "metadata": {
        "id": "85f17084",
        "outputId": "221219b6-5169-4a09-fcde-0084cfa45bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should look up the temperature and then use the calculator\n",
            "Action: Search\n",
            "Action Input: \"High temperature in Nashville yesterday\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh: 71.6ºf @3:55 PM Low: 46.04ºf @5:53 AM Approx.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use the calculator to raise the number to the 0.23 power\n",
            "Action: Calculator\n",
            "Action Input: 71.6^0.23\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.6707300633755717\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The high temperature in Nashville yesterday in Fahrenheit raised to the 0.23 power is 2.6707300633755717.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The high temperature in Nashville yesterday in Fahrenheit raised to the 0.23 power is 2.6707300633755717.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.run(\"What was the high temperature in Nashville yesterday in Fahrenheit? What is that number raised to the 0.23 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7807426f",
      "metadata": {
        "id": "7807426f"
      },
      "source": [
        "## Memory and State Changes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use special chains (such as the ConversationChain) to allow your model to remember past inputs."
      ],
      "metadata": {
        "id": "JALS2Ub6Lqyl"
      },
      "id": "JALS2Ub6Lqyl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b808ba48",
      "metadata": {
        "id": "b808ba48"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(llm = llm, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c1ae80",
      "metadata": {
        "id": "a1c1ae80",
        "outputId": "b5e179fb-d891-45fe-ed72-8ae393e23aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" Hi there! It's so nice to meet you. My name is AI. What's yours?\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input = \"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8553905",
      "metadata": {
        "id": "f8553905",
        "outputId": "00afb71a-97ec-4579-c141-68339fd2f267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's so nice to meet you. My name is AI. What's yours?\n",
            "Human: My name is Myranda.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Nice to meet you, Myranda! What can I do for you today?'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input = \"My name is Myranda.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2f02c4",
      "metadata": {
        "id": "9e2f02c4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}