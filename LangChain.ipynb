{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myrandaGoesToSpace/langchain/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a8ac011",
      "metadata": {
        "id": "2a8ac011"
      },
      "source": [
        "# LangChain Examples\n",
        "\n",
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f9676c",
      "metadata": {
        "id": "76f9676c"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import ConversationChain\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8681991",
      "metadata": {
        "id": "b8681991"
      },
      "outputs": [],
      "source": [
        "path_to_key = \"./openai-api-key.txt\"\n",
        "\n",
        "with open(path_to_key) as fo:\n",
        "    key = fo.readline()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = key.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7221ad84",
      "metadata": {
        "id": "7221ad84"
      },
      "source": [
        "## Get predictions from language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c424521",
      "metadata": {
        "id": "6c424521"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71187944",
      "metadata": {
        "id": "71187944",
        "outputId": "50592a9c-c172-44c8-abb4-aacc3d9470a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "BrightSox\n"
          ]
        }
      ],
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006bca95",
      "metadata": {
        "id": "006bca95"
      },
      "source": [
        "## Manage Prompts for LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574eacdc",
      "metadata": {
        "id": "574eacdc"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"product\"],\n",
        "    template = \"What is a good name for a company that makes {product}?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a279ce87",
      "metadata": {
        "id": "a279ce87",
        "outputId": "88ce9ac0-9ecc-4db4-8341-2e98400b724c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is a good name for a company that makes modern clothing?\n"
          ]
        }
      ],
      "source": [
        "new_prompt = prompt.format(product=\"modern clothing\")\n",
        "print(new_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9d0c6a",
      "metadata": {
        "id": "3a9d0c6a",
        "outputId": "f82afa85-04a5-465a-9c5e-f83c03c30413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ThreadStyles.\n"
          ]
        }
      ],
      "source": [
        "print(llm(new_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4548164",
      "metadata": {
        "id": "b4548164"
      },
      "source": [
        "## Using chains to create a workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af1e212",
      "metadata": {
        "id": "5af1e212"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede52d9e",
      "metadata": {
        "id": "ede52d9e",
        "outputId": "faea39a8-306e-4834-8cff-3f2a0c6f6bed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nSplashy Sox.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"colorful socks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb86f047",
      "metadata": {
        "id": "bb86f047"
      },
      "source": [
        "## Agents to Dynamically Call Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d81fb7a",
      "metadata": {
        "id": "5d81fb7a"
      },
      "outputs": [],
      "source": [
        "# Get SerpAPI package and API keys\n",
        "path_to_key = \"./serp-api-key.txt\"\n",
        "\n",
        "with open(path_to_key) as fo:\n",
        "    key = fo.readline()\n",
        "\n",
        "os.environ[\"SERPAPI_API_KEY\"] = key.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbd6863",
      "metadata": {
        "id": "0dbd6863"
      },
      "outputs": [],
      "source": [
        "# Language model already loaded in previous cells\n",
        "# Now we add tools to the llm\n",
        "\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c897ee",
      "metadata": {
        "id": "05c897ee"
      },
      "outputs": [],
      "source": [
        "# initialize agent with tools, llm, and agent type\n",
        "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f17084",
      "metadata": {
        "id": "85f17084",
        "outputId": "14e1b900-672d-4bcd-b2ba-43d775230003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should look up the temperature and then use the calculator\n",
            "Action: Search\n",
            "Action Input: \"High temperature in Nashville yesterday\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh: 71.6ºf @3:55 PM Low: 46.04ºf @5:53 AM Approx.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use the calculator to raise the number to the 0.23 power\n",
            "Action: Calculator\n",
            "Action Input: 71.6^0.23\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.6707300633755717\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The high temperature in Nashville yesterday in Fahrenheit raised to the 0.23 power is 2.6707300633755717.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The high temperature in Nashville yesterday in Fahrenheit raised to the 0.23 power is 2.6707300633755717.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.run(\"What was the high temperature in Nashville yesterday in Fahrenheit? What is that number raised to the 0.23 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7807426f",
      "metadata": {
        "id": "7807426f"
      },
      "source": [
        "## Memory and State Changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b808ba48",
      "metadata": {
        "id": "b808ba48"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(llm = llm, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c1ae80",
      "metadata": {
        "id": "a1c1ae80",
        "outputId": "60af0470-49e2-4597-bff6-7c6a079c6e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" Hi there! It's so nice to meet you. My name is AI. What's yours?\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input = \"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8553905",
      "metadata": {
        "id": "f8553905",
        "outputId": "a08b5b3e-8566-4261-9379-2bf75ba6812f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's so nice to meet you. My name is AI. What's yours?\n",
            "Human: My name is Myranda.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Nice to meet you, Myranda! What can I do for you today?'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input = \"My name is Myranda.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2f02c4",
      "metadata": {
        "id": "9e2f02c4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}